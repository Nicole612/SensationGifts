"""
ðŸš€ VOLLSTÃ„NDIGE AI-PROCESSING-PIPELINE TESTS
============================================

Detaillierte Tests fÃ¼r:
1. IntelligentProcessor
2. OptimizationEngine
3. ModelSelector
4. Async Clients
5. Performance Tracking
6. End-to-End Integration
"""

import asyncio
import time
from datetime import datetime
from typing import Dict, Any, List
import json
from unittest.mock import Mock, patch, AsyncMock

# Optional: pytest fÃ¼r erweiterte Features
try:
    import pytest
    HAS_PYTEST = True
except ImportError:
    HAS_PYTEST = False
    # Mock pytest fÃ¼r einfache Tests
    class pytest:
        @staticmethod
        def mark_asyncio(f):
            return f

# AI Engine Imports
from ai_engine.processors import IntelligentProcessor
from ai_engine.processors.optimization_engine import OptimizationObjective
from ai_engine.processors.model_selector import ModelSelector, SelectionStrategy
from ai_engine.processors.response_parser import ResponseParser, ParsingStrategy
from ai_engine.schemas.input_schemas import (
    GiftRecommendationRequest,
    PromptMethodInput,
    PersonalityMethodInput
)
from ai_engine.schemas.output_schemas import GiftRecommendationResponse
from ai_engine.schemas.prompt_schemas import AIModelType

# Flask App Imports
from app.routes.gift_finder import (
    get_intelligent_processor,
    convert_prompt_input_to_gift_request,
    convert_personality_input_to_gift_request,
    build_context_for_intelligent_processor,
    determine_optimization_preference,
    run_async,
    execute_ai_model_call_async
)


# =============================================================================
# ðŸ§ª UNIT TESTS: IntelligentProcessor
# =============================================================================

class TestIntelligentProcessor:
    """Unit Tests fÃ¼r IntelligentProcessor"""
    
    def test_processor_initialization(self):
        """Test: IntelligentProcessor initialisiert korrekt"""
        processor = IntelligentProcessor()
        
        assert processor is not None
        assert processor.prompt_builder is not None
        assert processor.response_parser is not None
        assert processor.model_selector is not None
        assert processor.optimization_engine is not None
        assert processor.processing_stats is not None
        
        print("âœ… IntelligentProcessor initialisiert korrekt")
    
    def test_processor_singleton(self):
        """Test: Singleton Pattern funktioniert"""
        processor1 = get_intelligent_processor()
        processor2 = get_intelligent_processor()
        
        # Sollte dieselbe Instanz sein
        assert processor1 is processor2
        print("âœ… Singleton Pattern funktioniert")
    
    async def test_process_gift_recommendation_request(self):
        """Test: process_gift_recommendation_request funktioniert"""
        processor = IntelligentProcessor()
        
        # Erstelle Test-Request (mit gÃ¼ltigen Daten)
        request = GiftRecommendationRequest(
            personality_data={
                "user_prompt": "Ich suche ein Geschenk fÃ¼r meine Freundin, die gerne reist und BÃ¼cher liest.",
                "prompt_method": True
            },
            occasion="birthday",
            relationship="friend",
            budget_min=10,
            budget_max=100,
            number_of_recommendations=3
        )
        
        # Test mit BALANCED_ROI
        result = await processor.process_gift_recommendation_request(
            request=request,
            optimization_preference=OptimizationObjective.BALANCED_ROI,
            context={"test": True}
        )
        
        assert result is not None
        # PrÃ¼fe dass result ein Dictionary ist mit den erwarteten Keys
        assert isinstance(result, dict)
        # PrÃ¼fe Pipeline Configuration (kann in verschiedenen Formaten sein)
        if "pipeline_configuration" in result:
            pipeline_config = result["pipeline_configuration"]
            assert isinstance(pipeline_config, dict)
        # PrÃ¼fe Execution Plan
        if "execution_plan" in result:
            execution_plan = result["execution_plan"]
            assert isinstance(execution_plan, dict)
            # PrÃ¼fe dass final_prompt oder target_model vorhanden ist
            assert "final_prompt" in execution_plan or "target_model" in execution_plan
        
        print("âœ… process_gift_recommendation_request funktioniert")
        if "execution_plan" in result:
            execution_plan = result["execution_plan"]
            print(f"   - Selected Model: {execution_plan.get('target_model')}")
        if "pipeline_configuration" in result:
            pipeline_config = result["pipeline_configuration"]
            print(f"   - Optimization Strategy: {pipeline_config.get('optimization_metadata', {}).get('strategy', 'N/A')}")
    
    async def test_parse_ai_response(self):
        """Test: parse_ai_response funktioniert"""
        processor = IntelligentProcessor()
        
        # Mock AI Response
        mock_response = """
        {
            "recommendations": [
                {
                    "title": "Test Geschenk",
                    "description": "Ein Test-Geschenk",
                    "price_range": "â‚¬25-â‚¬50",
                    "emotional_impact": "Freude",
                    "confidence_score": 0.9
                }
            ],
            "overall_strategy": "Test Strategy",
            "overall_confidence": 0.85
        }
        """
        
        parsed = processor.parse_ai_response(
            raw_response=mock_response,
            source_model=AIModelType.OPENAI_GPT4,
            expected_schema=GiftRecommendationResponse,
            parsing_strategy=ParsingStrategy.HYBRID_PARSING
        )
        
        assert parsed is not None
        # PrÃ¼fe dass parsing entweder erfolgreich war oder structured_output vorhanden ist
        assert hasattr(parsed, 'parsing_success') or hasattr(parsed, 'structured_output') or hasattr(parsed, 'parsed_data')
        
        print("âœ… parse_ai_response funktioniert")
    
    def test_record_actual_performance(self):
        """Test: Performance Tracking funktioniert"""
        processor = IntelligentProcessor()
        
        # Mock Processing Result
        processing_result = {
            "pipeline_configuration": {
                "optimization_metadata": {"strategy": "balanced"}
            },
            "model_selection": {
                "selected_model": AIModelType.OPENAI_GPT4
            },
            "execution_plan": {
                "target_model": AIModelType.OPENAI_GPT4
            }
        }
        
        # Mock Parsed Response
        parsed_response = Mock()
        parsed_response.parsing_success = True
        parsed_response.confidence_score = 0.9
        
        # Mock Actual Metrics als AIModelPerformanceMetrics Objekt
        from ai_engine.schemas.output_schemas import AIModelPerformanceMetrics
        from datetime import datetime
        
        actual_metrics = AIModelPerformanceMetrics(
            model_name=AIModelType.OPENAI_GPT4.value,
            timestamp=datetime.now(),
            response_time_ms=1500,
            tokens_used=500,  # âœ… FIX: tokens_used nicht token_usage
            cost_estimate=0.02,
            output_quality_score=0.9,
            had_errors=False,
            user_satisfaction_predicted=0.85,
            # âœ… FIX: Required fields hinzufÃ¼gen
            request_successful=True,
            request_type="gift_recommendation",
            complexity_level="moderate",
            optimization_goal="quality"
        )
        
        # Record Performance
        processor.record_actual_performance(
            processing_result=processing_result,
            actual_response=parsed_response,
            actual_metrics=actual_metrics
        )
        
        # PrÃ¼fe Statistics
        assert processor.processing_stats["total_requests"] > 0
        
        print("âœ… Performance Tracking funktioniert")
    
    def test_get_processing_analytics(self):
        """Test: Analytics werden korrekt zurÃ¼ckgegeben"""
        processor = IntelligentProcessor()
        
        analytics = processor.get_processing_analytics()
        
        assert analytics is not None
        assert "processing_statistics" in analytics
        assert "optimization_status" in analytics
        assert "model_health" in analytics
        assert "parsing_statistics" in analytics
        assert "overall_system_health" in analytics
        
        print("âœ… Analytics werden korrekt zurÃ¼ckgegeben")
        print(f"   - System Health: {analytics.get('overall_system_health', {}).get('status')}")


# =============================================================================
# ðŸ§ª UNIT TESTS: OptimizationEngine
# =============================================================================

class TestOptimizationEngine:
    """Unit Tests fÃ¼r OptimizationEngine"""
    
    async def test_optimize_request_pipeline(self):
        """Test: Pipeline-Optimierung funktioniert"""
        from ai_engine.processors.optimization_engine import OptimizationEngine
        
        processor = IntelligentProcessor()
        optimization_engine = processor.optimization_engine
        
        # Test Request
        request = GiftRecommendationRequest(
            personality_data={"user_prompt": "Test", "prompt_method": True},
            occasion="birthday",
            relationship="friend",
            number_of_recommendations=3
        )
        
        # Test verschiedene Optimization Objectives
        for objective in [
            OptimizationObjective.BALANCED_ROI,
            OptimizationObjective.PERFORMANCE_MAXIMIZATION,
            OptimizationObjective.COST_EFFICIENCY,
            OptimizationObjective.USER_SATISFACTION
        ]:
            pipeline_config = await optimization_engine.optimize_request_pipeline(
                request=request,
                context={"test": True},
                optimization_preference=objective
            )
            
            assert pipeline_config is not None
            assert "prompt_configuration" in pipeline_config
            assert "model_configuration" in pipeline_config
            assert "optimization_metadata" in pipeline_config
            
            print(f"âœ… Optimization {objective.value} funktioniert")
    
    def test_optimization_weights(self):
        """Test: Optimization Weights werden korrekt gesetzt"""
        processor = IntelligentProcessor()
        optimization_engine = processor.optimization_engine
        
        # PrÃ¼fe Default Weights
        weights = optimization_engine.optimization_weights
        assert "cost" in weights
        assert "quality" in weights
        assert "speed" in weights
        assert "satisfaction" in weights
        
        # Summe sollte ~1.0 sein
        total_weight = sum(weights.values())
        assert 0.9 <= total_weight <= 1.1
        
        print("âœ… Optimization Weights korrekt")
        print(f"   - Weights: {weights}")


# =============================================================================
# ðŸ§ª UNIT TESTS: ModelSelector
# =============================================================================

class TestModelSelector:
    """Unit Tests fÃ¼r ModelSelector"""
    
    def test_model_selector_initialization(self):
        """Test: ModelSelector initialisiert korrekt"""
        selector = ModelSelector()
        
        assert selector is not None
        assert selector.performance_history is not None
        
        print("âœ… ModelSelector initialisiert korrekt")
    
    def test_select_optimal_model(self):
        """Test: Model-Auswahl funktioniert"""
        selector = ModelSelector()
        
        request = GiftRecommendationRequest(
            personality_data={"user_prompt": "Test", "prompt_method": True},
            occasion="birthday",
            relationship="friend",
            number_of_recommendations=3
        )
        
        # Test verschiedene Optimization Goals
        for goal in ["speed", "quality", "cost", "balanced"]:
            result = selector.select_optimal_model(
                request=request,
                optimization_goal=goal,
                strategy=None,
                context={"test": True}
            )
            
            assert result is not None
            assert "selected_model" in result
            assert "selection_reasoning" in result
            
            print(f"âœ… Model Selection fÃ¼r '{goal}' funktioniert")
            print(f"   - Selected: {result.get('selected_model')}")
            print(f"   - Reasoning: {result.get('selection_reasoning', 'N/A')[:100]}")


# =============================================================================
# ðŸ§ª INTEGRATION TESTS: VollstÃ¤ndige Pipeline
# =============================================================================

class TestCompletePipeline:
    """Integration Tests fÃ¼r vollstÃ¤ndige Pipeline"""
    
    async def test_full_pipeline_prompt_method(self):
        """Test: VollstÃ¤ndige Pipeline fÃ¼r Prompt-Method"""
        processor = IntelligentProcessor()
        
        # 1. Request erstellen
        prompt_input = PromptMethodInput(
            user_prompt="Ich suche ein Geschenk fÃ¼r meine Freundin, die gerne reist und BÃ¼cher liest.",
            occasion="birthday",
            relationship="friend",
            budget_min=20,
            budget_max=80
        )
        
        gift_request = convert_prompt_input_to_gift_request(
            prompt_input,
            {"optimization_goal": "quality"}
        )
        
        # 2. Context bauen
        context = build_context_for_intelligent_processor(
            {"optimization_goal": "quality"},
            "prompt",
            {"user_prompt": prompt_input.user_prompt}
        )
        
        # 3. Optimization Preference bestimmen
        optimization_pref = determine_optimization_preference(
            {"optimization_goal": "quality"}
        )
        
        # 4. Pipeline ausfÃ¼hren
        processing_result = await processor.process_gift_recommendation_request(
            request=gift_request,
            optimization_preference=optimization_pref,
            context=context
        )
        
        # 5. Validierung
        assert processing_result is not None
        execution_plan = processing_result.get("execution_plan", {})
        assert execution_plan.get("final_prompt") is not None
        assert execution_plan.get("target_model") is not None
        
        print("âœ… VollstÃ¤ndige Pipeline fÃ¼r Prompt-Method funktioniert")
        print(f"   - Model: {execution_plan.get('target_model')}")
        print(f"   - Prompt Length: {len(execution_plan.get('final_prompt', ''))}")
    
    async def test_full_pipeline_personality_method(self):
        """Test: VollstÃ¤ndige Pipeline fÃ¼r Personality-Method"""
        processor = IntelligentProcessor()
        
        # 1. Request erstellen
        from ai_engine.schemas.input_schemas import BigFiveScore, GiftPreferences, RelationshipType
        
        personality_scores = BigFiveScore(
            openness=4.5,
            conscientiousness=3.8,
            extraversion=4.2,
            agreeableness=4.0,
            neuroticism=2.5
        )
        
        gift_prefs = GiftPreferences(
            budget_min=30,
            budget_max=120
        )
        
        personality_input = PersonalityMethodInput(
            personality_scores=personality_scores,
            occasion="birthday",
            relationship_to_giver=RelationshipType.FRIEND_CLOSE,
            gift_preferences=gift_prefs
        )
        
        gift_request = convert_personality_input_to_gift_request(
            personality_input,
            {"optimization_goal": "quality"}  # âœ… FIX: "balanced" ist nicht erlaubt, verwende "quality"
        )
        
        # 2. Pipeline ausfÃ¼hren
        optimization_pref = determine_optimization_preference(
            {"optimization_goal": "quality"}  # âœ… FIX: "balanced" ist nicht erlaubt
        )
        
        processing_result = await processor.process_gift_recommendation_request(
            request=gift_request,
            optimization_preference=optimization_pref,
            context={"method_type": "personality"}
        )
        
        # 3. Validierung
        assert processing_result is not None
        execution_plan = processing_result.get("execution_plan", {})
        assert execution_plan.get("final_prompt") is not None
        
        print("âœ… VollstÃ¤ndige Pipeline fÃ¼r Personality-Method funktioniert")


# =============================================================================
# ðŸ§ª PERFORMANCE TESTS: Async Clients
# =============================================================================

class TestAsyncClients:
    """Performance Tests fÃ¼r Async Clients"""
    
    async def test_async_client_performance(self):
        """Test: Async Clients sind schneller als Sync"""
        from ai_engine.models.base_client import AIRequest, ResponseFormat
        
        # Test Prompt
        test_prompt = "Erstelle 3 Geschenkempfehlungen fÃ¼r einen Freund zum Geburtstag."
        
        # Mock Async Client (wir testen nicht die echte API)
        async def mock_async_call():
            await asyncio.sleep(0.1)  # Simuliere 100ms API Call
            return "Mock Response"
        
        # Mock Sync Call
        def mock_sync_call():
            time.sleep(0.1)  # Simuliere 100ms API Call
            return "Mock Response"
        
        # Test Async Performance
        async_start = time.time()
        results = await asyncio.gather(*[mock_async_call() for _ in range(5)])
        async_time = time.time() - async_start
        
        # Test Sync Performance
        sync_start = time.time()
        results_sync = [mock_sync_call() for _ in range(5)]
        sync_time = time.time() - sync_start
        
        print(f"âœ… Async Performance Test:")
        print(f"   - Async (5 calls): {async_time:.3f}s")
        print(f"   - Sync (5 calls): {sync_time:.3f}s")
        print(f"   - Speedup: {sync_time/async_time:.2f}x")
        
        # Async sollte deutlich schneller sein bei parallelen Calls
        assert async_time < sync_time * 0.8  # Mindestens 20% schneller


# =============================================================================
# ðŸ§ª END-TO-END TESTS: gift_finder.py Integration
# =============================================================================

class TestGiftFinderIntegration:
    """End-to-End Tests fÃ¼r gift_finder.py"""
    
    def test_helper_functions(self):
        """Test: Helper Functions funktionieren"""
        # Test convert_prompt_input_to_gift_request (min 10 Zeichen fÃ¼r user_prompt)
        prompt_input = PromptMethodInput(
            user_prompt="Ich suche ein Geschenk fÃ¼r meine Freundin zum Geburtstag.",
            occasion="birthday",
            relationship="friend",
            budget_min=10,
            budget_max=50
        )
        
        gift_request = convert_prompt_input_to_gift_request(
            prompt_input,
            {"optimization_goal": "quality"}
        )
        
        assert gift_request is not None
        assert gift_request.occasion == "birthday"
        assert gift_request.relationship == "friend"
        
        # Test build_context_for_intelligent_processor
        context = build_context_for_intelligent_processor(
            {"optimization_goal": "quality"},
            "prompt",
            {"user_prompt": "Test"}
        )
        
        assert context is not None
        assert context["method_type"] == "prompt"
        
        # Test determine_optimization_preference
        pref = determine_optimization_preference({"optimization_goal": "quality"})
        assert pref == OptimizationObjective.USER_SATISFACTION
        
        print("âœ… Helper Functions funktionieren")
    
    async def test_execute_ai_model_call_async_mock(self):
        """Test: execute_ai_model_call_async mit Mock"""
        # Mock AI Model Call - Patch the actual function
        from app.routes import gift_finder
        from ai_engine.schemas.prompt_schemas import AIModelType
        
        with patch.object(gift_finder, 'execute_ai_model_call_async', new_callable=AsyncMock) as mock_execute:
            # Setup mock return value
            mock_execute.return_value = '{"recommendations": [{"title": "Test"}]}'
            
            # Test the function
            result = await gift_finder.execute_ai_model_call_async(
                prompt="Test prompt",
                model=AIModelType.OPENAI_GPT4,
                fallback_chain=[AIModelType.GROQ_MIXTRAL],
                options={}
            )
            
            assert result is not None
            assert "recommendations" in result or isinstance(result, str)
            
            # Verify mock was called
            assert mock_execute.called
            
            print("âœ… execute_ai_model_call_async Mock funktioniert")


# =============================================================================
# ðŸ§ª ERROR HANDLING TESTS
# =============================================================================

class TestErrorHandling:
    """Tests fÃ¼r Error Handling"""
    
    async def test_processor_fallback(self):
        """Test: Fallback bei Processor-Fehler"""
        processor = IntelligentProcessor()
        
        # Invalid Request (sollte Fallback verwenden)
        invalid_request = GiftRecommendationRequest(
            personality_data={},
            occasion="",
            relationship="",
            number_of_recommendations=3
        )
        
        try:
            result = await processor.process_gift_recommendation_request(
                request=invalid_request,
                optimization_preference=OptimizationObjective.BALANCED_ROI
            )
            
            # Sollte trotzdem ein Result zurÃ¼ckgeben (Fallback)
            assert result is not None
            print("âœ… Fallback bei Processor-Fehler funktioniert")
        except Exception as e:
            # Fallback sollte Exception fangen
            print(f"âš ï¸ Exception (erwartet bei invalid request): {e}")


# =============================================================================
# ðŸ§ª PERFORMANCE BENCHMARK TESTS
# =============================================================================

class TestPerformanceBenchmarks:
    """Performance Benchmark Tests"""
    
    async def test_pipeline_performance(self):
        """Test: Pipeline Performance Benchmarks"""
        processor = IntelligentProcessor()
        
        request = GiftRecommendationRequest(
            personality_data={"user_prompt": "Test", "prompt_method": True},
            occasion="birthday",
            relationship="friend",
            number_of_recommendations=3
        )
        
        # Test Performance
        times = []
        for i in range(3):
            start = time.time()
            result = await processor.process_gift_recommendation_request(
                request=request,
                optimization_preference=OptimizationObjective.BALANCED_ROI
            )
            elapsed = time.time() - start
            times.append(elapsed)
        
        avg_time = sum(times) / len(times)
        print(f"âœ… Pipeline Performance:")
        print(f"   - Average Time: {avg_time:.3f}s")
        print(f"   - Min Time: {min(times):.3f}s")
        print(f"   - Max Time: {max(times):.3f}s")
        
        # Performance sollte akzeptabel sein (< 5s)
        assert avg_time < 5.0


# =============================================================================
# ðŸ§ª MAIN TEST RUNNER
# =============================================================================

def run_all_tests():
    """FÃ¼hrt alle Tests aus"""
    print("=" * 80)
    print("ðŸš€ VOLLSTÃ„NDIGE AI-PROCESSING-PIPELINE TESTS")
    print("=" * 80)
    print()
    
    # Test Suites
    test_suites = [
        ("IntelligentProcessor", TestIntelligentProcessor),
        ("OptimizationEngine", TestOptimizationEngine),
        ("ModelSelector", TestModelSelector),
        ("Complete Pipeline", TestCompletePipeline),
        ("Async Clients", TestAsyncClients),
        ("Gift Finder Integration", TestGiftFinderIntegration),
        ("Error Handling", TestErrorHandling),
        ("Performance Benchmarks", TestPerformanceBenchmarks)
    ]
    
    results = {}
    
    for suite_name, test_class in test_suites:
        print(f"\n{'='*80}")
        print(f"ðŸ§ª Testing: {suite_name}")
        print(f"{'='*80}")
        
        # Run tests
        test_instance = test_class()
        test_methods = [method for method in dir(test_instance) if method.startswith('test_')]
        
        passed = 0
        failed = 0
        
        for method_name in test_methods:
            try:
                method = getattr(test_instance, method_name)
                if asyncio.iscoroutinefunction(method):
                    asyncio.run(method())
                else:
                    method()
                passed += 1
            except Exception as e:
                print(f"âŒ {method_name} FAILED: {e}")
                failed += 1
        
        results[suite_name] = {"passed": passed, "failed": failed}
        print(f"\nâœ… {suite_name}: {passed} passed, {failed} failed")
    
    # Summary
    print("\n" + "=" * 80)
    print("ðŸ“Š TEST SUMMARY")
    print("=" * 80)
    
    total_passed = sum(r["passed"] for r in results.values())
    total_failed = sum(r["failed"] for r in results.values())
    
    for suite_name, result in results.items():
        print(f"{suite_name:30} âœ… {result['passed']:3} âŒ {result['failed']:3}")
    
    print("-" * 80)
    print(f"{'TOTAL':30} âœ… {total_passed:3} âŒ {total_failed:3}")
    print("=" * 80)
    
    return total_failed == 0


if __name__ == "__main__":
    success = run_all_tests()
    exit(0 if success else 1)

