"""
Async Groq Client - Ultra-Fast Async AI f√ºr Concurrent Gift Recommendations

ASYNC SUPERPOWERS:
- Concurrent processing of multiple requests
- 10x faster for bulk operations
- Perfect for real-time user interfaces
- Handles batch gift recommendations

PERFORMANCE BOOST:
- 1 recommendation: ~0.5 seconds (same as sync)
- 10 recommendations: ~1 second (instead of 5 seconds sync!)
- 50 recommendations: ~3 seconds (instead of 25 seconds sync!)

USE CASES:
- Family gift recommendations (all at once)
- Real-time suggestion streams
- Bulk processing for events
- Live chat with instant responses
"""

import asyncio
import json
import time
from datetime import datetime
from typing import Dict, List, Optional, Any

# Async Groq Support
try:
    from groq import AsyncGroq  # Async version
except ImportError:
    raise ImportError("Async Groq library not available. Update: pip install groq>=0.4.0")

# Import from async_base_client.py
from .async_base_client import AsyncBaseAIClient, AIRequest, AIResponse
from .base_client import (
    AIModelType, ModelCapability, ResponseFormat, 
    GiftRecommendationSchema
)


class AsyncGroqClient(AsyncBaseAIClient):
    """
    Async Groq Client f√ºr Ultra-Fast Concurrent Gift Recommendations
    
    FEATURES:
    - Async Mixtral-8x7B & Llama-3 Support
    - Concurrent batch processing
    - Real-time suggestion streaming
    - Sub-second response times
    - Cost-efficient operations
    
    PERFORMANCE FOCUS:
    - Maximum concurrency for bulk operations
    - Real-time user interface support
    - Instant alternative generation
    - Live gift brainstorming
    """
    
    def __init__(self, 
                 api_key: str,
                 model_type: AIModelType = AIModelType.GROQ_MIXTRAL,
                 rate_limit_per_minute: int = 300,
                 max_concurrent_requests: int = 20):  # Higher concurrency for Groq
        
        # Validate model type
        if model_type not in [AIModelType.GROQ_MIXTRAL, AIModelType.GROQ_LLAMA]:
            raise ValueError(f"Invalid model type for AsyncGroq: {model_type}")
        
        # Initialize parent (AsyncBaseAIClient)
        super().__init__(api_key, model_type, rate_limit_per_minute, max_concurrent_requests)
        
        # Async Groq Client
        self.client = AsyncGroq(api_key=api_key)
        
        # Model configuration
        self.model_name = self._get_model_name()
        self.max_tokens_limit = self._get_max_tokens()
        
        print(f"‚ö° AsyncGroqClient initialized: {self.model_name}")
    
    def _get_model_name(self) -> str:
        """Maps model type to Groq model name"""
        mapping = {
            AIModelType.GROQ_MIXTRAL: "mixtral-8x7b-32768",
            AIModelType.GROQ_LLAMA: "llama3-70b-8192"
        }
        return mapping[self.model_type]
    
    def _get_max_tokens(self) -> int:
        """Get max tokens for this model"""
        limits = {
            AIModelType.GROQ_MIXTRAL: 32768,
            AIModelType.GROQ_LLAMA: 8192
        }
        return limits[self.model_type]
    
    def _get_model_capabilities(self) -> List[ModelCapability]:
        """Define capabilities (gleich wie sync)"""
        capabilities = [
            ModelCapability.TEXT_GENERATION,
            ModelCapability.FAST_RESPONSE,
            ModelCapability.COST_EFFICIENT
        ]
        
        if self.model_type == AIModelType.GROQ_MIXTRAL:
            capabilities.append(ModelCapability.JSON_OUTPUT)
        
        return capabilities
    
    def _get_model_pricing(self) -> Dict[str, float]:
        """Groq pricing (very cost-efficient!)"""
        pricing = {
            AIModelType.GROQ_MIXTRAL: {
                "input_per_token": 0.00000027,   # $0.27 per 1M tokens
                "output_per_token": 0.00000027   # Same for output
            },
            AIModelType.GROQ_LLAMA: {
                "input_per_token": 0.00000059,   # $0.59 per 1M tokens
                "output_per_token": 0.00000079   # $0.79 per 1M tokens
            }
        }
        return pricing[self.model_type]
    
    # === CORE ASYNC METHOD ===
    
    async def _make_async_api_call(self, request: AIRequest) -> AIResponse:
        """
        Async Groq API Call - Ultra-fast with concurrency support
        """
        
        try:
            # Build messages (gleich wie sync)
            messages = []
            if request.system_prompt:
                messages.append({"role": "system", "content": request.system_prompt})
            messages.append({"role": "user", "content": request.prompt})
            
            # Handle JSON mode
            if request.response_format == ResponseFormat.JSON:
                if "json" not in request.prompt.lower():
                    messages[-1]["content"] += "\n\nIMPORTANT: Respond with valid JSON only, no other text."
            
            # API parameters
            api_params = {
                "model": self.model_name,
                "messages": messages,
                "temperature": request.temperature,
                "top_p": request.top_p,
                "max_tokens": min(request.max_tokens or 1000, self.max_tokens_limit),
                "stream": False
            }
            
            # üî• ASYNC GROQ CALL - runs concurrently with other calls!
            print(f"‚ö° Making async Groq API call...")
            start_time = time.time()
            
            # ASYNC CALL - doesn't block other operations!
            response = await self.client.chat.completions.create(**api_params)
            
            response_time = time.time() - start_time
            print(f"‚úÖ Async Groq response received in {response_time:.3f}s")
            
            # Extract response data (gleich wie sync)
            content = response.choices[0].message.content
            tokens_used = response.usage.total_tokens
            
            # Calculate cost
            input_tokens = response.usage.prompt_tokens
            output_tokens = response.usage.completion_tokens
            cost = (
                input_tokens * self.pricing["input_per_token"] +
                output_tokens * self.pricing["output_per_token"]
            )
            
            # Parse JSON if requested
            parsed_json = None
            if request.response_format == ResponseFormat.JSON:
                try:
                    # Groq sometimes adds extra text, try to extract JSON
                    json_start = content.find('{')
                    json_end = content.rfind('}') + 1
                    if json_start >= 0 and json_end > json_start:
                        json_content = content[json_start:json_end]
                        parsed_json = json.loads(json_content)
                    else:
                        parsed_json = json.loads(content)
                except json.JSONDecodeError as e:
                    return AIResponse(
                        content=content,
                        model_type=self.model_type,
                        tokens_used=tokens_used,
                        cost=cost,
                        response_time=response_time,
                        timestamp=datetime.now(),
                        success=False,
                        error=f"Invalid JSON response: {e}"
                    )
            
            # Success response
            return AIResponse(
                content=content,
                model_type=self.model_type,
                tokens_used=tokens_used,
                cost=cost,
                response_time=response_time,
                timestamp=datetime.now(),
                success=True,
                parsed_json=parsed_json
            )
            
        except Exception as e:
            print(f"‚ùå Async Groq API error: {e}")
            return AIResponse(
                content="",
                model_type=self.model_type,
                tokens_used=0,
                cost=0.0,
                response_time=0.0,
                timestamp=datetime.now(),
                success=False,
                error=f"Async Groq API Error: {e}"
            )
    
    # === ASYNC SPEED-OPTIMIZED METHODS ===
    
    async def quick_gift_suggestions_async(self,
                                         personality_summary: str,
                                         occasion: str,
                                         budget: str,
                                         max_suggestions: int = 5) -> List[str]:
        """
        Async version of quick gift suggestions
        
        VERWENDUNG:
        suggestions = await client.quick_gift_suggestions_async(
            "creative person who loves photography",
            "birthday",
            "50-100",
            5
        )
        """
        
        print(f"‚ö° Generating {max_suggestions} quick suggestions async...")
        
        prompt = f"""
        Quick gift ideas for:
        Person: {personality_summary}
        Occasion: {occasion}
        Budget: ‚Ç¨{budget}
        
        Give {max_suggestions} specific, creative gift ideas.
        Keep each suggestion under 10 words.
        Focus on practical gifts they'd actually love.
        
        Format as JSON: {{"suggestions": ["idea1", "idea2", "idea3"]}}
        """
        
        response = await self.generate_json_async(
            prompt=prompt,
            json_schema={"type": "object", "properties": {"suggestions": {"type": "array"}}},
            system_prompt="You are a fast gift consultant. Quick, practical suggestions only.",
            temperature=0.7
        )
        
        if response.success and response.parsed_json:
            return response.parsed_json.get("suggestions", [])
        return []
    
    async def instant_alternatives_async(self, 
                                       base_gift: str,
                                       personality_hint: str,
                                       price_range: str,
                                       num_alternatives: int = 5) -> List[str]:
        """
        Instant alternative suggestions (async)
        
        VERWENDUNG:
        alternatives = await client.instant_alternatives_async(
            "Camera", "creative photographer", "100-200", 5
        )
        """
        
        prompt = f"""
        Base gift: "{base_gift}"
        Person: {personality_hint}
        Budget: ‚Ç¨{price_range}
        
        {num_alternatives} quick alternatives that are:
        - Similar emotional appeal
        - Different product category
        - Same price range
        - More personalized
        
        JSON: {{"alternatives": ["alt1", "alt2", "alt3", "alt4", "alt5"]}}
        """
        
        response = await self.generate_json_async(
            prompt=prompt,
            json_schema={"type": "object", "properties": {"alternatives": {"type": "array"}}},
            temperature=0.8,
            system_prompt="Creative gift consultant. Think outside the box but stay practical."
        )
        
        if response.success and response.parsed_json:
            return response.parsed_json.get("alternatives", [])
        return []
    
    async def fast_gift_recommendation_async(self,
                                           personality_profile: Dict,
                                           occasion: str,
                                           budget_range: str,
                                           relationship: str) -> GiftRecommendationSchema:
        """
        Async version of fast gift recommendation
        
        VERWENDUNG:
        async with AsyncGroqClient(api_key) as client:
            gift = await client.fast_gift_recommendation_async(
                personality_profile=profile,
                occasion="birthday",
                budget_range="‚Ç¨50-‚Ç¨150",
                relationship="friend"
            )
        """
        
        print(f"‚ö° Generating async fast gift recommendation for {occasion}...")
        
        prompt = f"""
        FAST GIFT RECOMMENDATION:
        
        Person: {personality_profile.get('summary', 'Creative, friendly person')}
        Traits: {personality_profile.get('personality_scores', {})}
        Hobbies: {personality_profile.get('hobbies', [])}
        Occasion: {occasion}
        Budget: ‚Ç¨{budget_range}
        Relationship: {relationship}
        
        Quick recommendation with:
        - Specific gift name
        - 1-sentence reasoning
        - Match score 0-1
        - Emotional appeal
        - 2-3 personalization ideas
        - Price range
        - 2-3 alternatives
        - Confidence 0-1
        
        Respond in this JSON structure exactly:
        {{
            "gift_name": "specific gift name",
            "reasoning": "one sentence why it fits",
            "match_score": 0.8,
            "emotional_appeal": "what emotion it triggers",
            "personalization_ideas": ["idea1", "idea2"],
            "price_range": "budget category",
            "alternative_gifts": ["alt1", "alt2"],
            "confidence": 0.9
        }}
        """
        
        response = await self.generate_json_async(
            prompt=prompt,
            json_schema=GiftRecommendationSchema.model_json_schema(),
            system_prompt="Fast gift expert. Quick but thoughtful recommendations.",
            temperature=0.6
        )
        
        if not response.success:
            # Fallback recommendation
            return GiftRecommendationSchema(
                gift_name="Personalized Photo Album",
                reasoning="Safe, thoughtful choice that works for most relationships",
                match_score=0.7,
                emotional_appeal="nostalgia",
                personalization_ideas=["Add favorite memories", "Custom cover design"],
                price_range=budget_range,
                alternative_gifts=["Custom mug", "Gift card"],
                confidence=0.6
            )
        
        try:
            recommendation = GiftRecommendationSchema.model_validate(response.parsed_json)
            print(f"‚úÖ Async fast gift recommendation: {recommendation.gift_name}")
            return recommendation
            
        except Exception as e:
            # Fallback on validation error
            return GiftRecommendationSchema(
                gift_name="Gift Card",
                reasoning="Flexible option when recommendation failed",
                match_score=0.5,
                emotional_appeal="choice",
                personalization_ideas=["Custom amount", "Favorite store"],
                price_range=budget_range,
                alternative_gifts=["Cash", "Experience voucher"],
                confidence=0.3
            )
    
    # === BATCH PROCESSING SUPERPOWERS ===
    
    async def batch_family_gift_recommendations_async(self,
                                                    family_members: List[Dict]) -> List[GiftRecommendationSchema]:
        """
        Process gift recommendations for entire family concurrently
        
        BEISPIEL:
        family = [
            {"name": "Mama", "profile": profile1, "occasion": "christmas", "budget": "‚Ç¨100-‚Ç¨200", "relationship": "mother"},
            {"name": "Papa", "profile": profile2, "occasion": "christmas", "budget": "‚Ç¨75-‚Ç¨150", "relationship": "father"},
            {"name": "Schwester", "profile": profile3, "occasion": "christmas", "budget": "‚Ç¨50-‚Ç¨100", "relationship": "sister"}
        ]
        
        gifts = await client.batch_family_gift_recommendations_async(family)
        # Alle 3 Empfehlungen in ~1 Sekunde statt 3 Sekunden!
        """
        
        print(f"üë®‚Äçüë©‚Äçüëß‚Äçüë¶ Processing {len(family_members)} family gift recommendations concurrently...")
        
        # Create tasks for all family members
        tasks = []
        for member in family_members:
            task = self.fast_gift_recommendation_async(
                personality_profile=member["profile"],
                occasion=member["occasion"],
                budget_range=member["budget"],
                relationship=member["relationship"]
            )
            tasks.append(task)
        
        # Execute all tasks concurrently
        start_time = time.time()
        recommendations = await asyncio.gather(*tasks, return_exceptions=True)
        total_time = time.time() - start_time
        
        # Process results and add family member names
        family_gifts = []
        for i, recommendation in enumerate(recommendations):
            if isinstance(recommendation, GiftRecommendationSchema):
                # Add family member metadata
                recommendation.metadata = {"family_member": family_members[i]["name"]}
                family_gifts.append(recommendation)
            elif isinstance(recommendation, Exception):
                print(f"‚ö†Ô∏è Failed to get recommendation for {family_members[i]['name']}: {recommendation}")
                # Add fallback recommendation
                fallback = GiftRecommendationSchema(
                    gift_name="Gift Card",
                    reasoning=f"Fallback for {family_members[i]['name']} due to error",
                    match_score=0.5,
                    emotional_appeal="choice",
                    personalization_ideas=["Custom amount"],
                    price_range=family_members[i]["budget"],
                    alternative_gifts=["Cash"],
                    confidence=0.3
                )
                fallback.metadata = {"family_member": family_members[i]["name"]}
                family_gifts.append(fallback)
        
        print(f"‚úÖ Family gift recommendations completed in {total_time:.2f}s")
        print(f"üìä Average time per recommendation: {total_time/len(family_members):.2f}s")
        
        return family_gifts
    
    async def real_time_suggestion_stream_async(self,
                                              personality_profile: Dict,
                                              occasion: str,
                                              budget_range: str,
                                              stream_count: int = 10) -> List[str]:
        """
        Generate stream of gift suggestions in real-time
        
        Perfect for live user interfaces where suggestions appear as user types
        
        VERWENDUNG:
        async for suggestion in client.real_time_suggestion_stream_async(profile, "birthday", "50-100"):
            print(f"New suggestion: {suggestion}")
        """
        
        print(f"üîÑ Starting real-time suggestion stream ({stream_count} suggestions)...")
        
        # Create multiple concurrent tasks for suggestions
        tasks = []
        for i in range(stream_count):
            prompt = f"""
            Quick gift suggestion #{i+1} for:
            Person: {personality_profile.get('summary', 'Unknown')}
            Occasion: {occasion}
            Budget: ‚Ç¨{budget_range}
            
            Give ONE specific, creative gift idea.
            Make it different from typical suggestions.
            
            JSON: {{"suggestion": "specific gift idea"}}
            """
            
            task = self.generate_json_async(
                prompt=prompt,
                json_schema={"type": "object", "properties": {"suggestion": {"type": "string"}}},
                temperature=0.8 + (i * 0.02),  # Slightly increase randomness for variety
                system_prompt="Creative gift consultant. Each suggestion should be unique."
            )
            tasks.append(task)
        
        # Execute all tasks concurrently
        responses = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Extract suggestions
        suggestions = []
        for response in responses:
            if isinstance(response, AIResponse) and response.success and response.parsed_json:
                suggestion = response.parsed_json.get("suggestion")
                if suggestion:
                    suggestions.append(suggestion)
        
        print(f"‚úÖ Generated {len(suggestions)} real-time suggestions")
        return suggestions
    
    async def validate_gift_ideas_batch_async(self,
                                            gift_ideas: List[str],
                                            personality_profile: Dict,
                                            occasion: str) -> List[Dict[str, Any]]:
        """
        Validate multiple gift ideas concurrently
        
        VERWENDUNG:
        validations = await client.validate_gift_ideas_batch_async(
            ["Camera", "Art Kit", "Spa Day"],
            profile,
            "birthday"
        )
        """
        
        print(f"‚úÖ Validating {len(gift_ideas)} gift ideas concurrently...")
        
        # Create validation tasks
        tasks = []
        for gift_idea in gift_ideas:
            prompt = f"""
            Gift idea: "{gift_idea}"
            Person: {personality_profile.get('summary', 'Unknown')}
            Occasion: {occasion}
            
            Quick validation:
            - Match score 1-10?
            - Why good/bad? (1 sentence)
            - Better alternative? (if score < 7)
            
            JSON: {{"score": 8, "reason": "explanation", "alternative": "better idea or null"}}
            """
            
            task = self.generate_json_async(
                prompt=prompt,
                json_schema={"type": "object"},
                temperature=0.3,
                system_prompt="Gift validation expert. Quick, honest assessments."
            )
            tasks.append(task)
        
        # Execute all validations concurrently
        responses = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Process results
        validations = []
        for i, response in enumerate(responses):
            if isinstance(response, AIResponse) and response.success and response.parsed_json:
                validation = response.parsed_json
                validation["gift_idea"] = gift_ideas[i]
                validations.append(validation)
            else:
                # Fallback validation
                validations.append({
                    "gift_idea": gift_ideas[i],
                    "score": 5,
                    "reason": "Could not validate",
                    "alternative": None
                })
        
        return validations
    
    # === PERFORMANCE TESTING ===
    
    async def async_speed_benchmark_async(self, num_concurrent: int = 10) -> Dict[str, Any]:
        """
        Benchmark async performance with concurrent requests
        
        VERWENDUNG:
        results = await client.async_speed_benchmark_async(10)
        """
        
        print(f"üöÄ Running async speed benchmark with {num_concurrent} concurrent requests...")
        
        # Create test tasks
        tasks = []
        for i in range(num_concurrent):
            task = self.generate_text_async(f"Creative gift idea #{i+1}", max_tokens=50)
            tasks.append(task)
        
        # Measure concurrent execution time
        start_time = time.time()
        responses = await asyncio.gather(*tasks, return_exceptions=True)
        total_time = time.time() - start_time
        
        # Analyze results
        successful_responses = [r for r in responses if isinstance(r, AIResponse) and r.success]
        
        results = {
            "concurrent_requests": num_concurrent,
            "successful_requests": len(successful_responses),
            "total_time": total_time,
            "avg_time_per_request": total_time / num_concurrent,
            "requests_per_second": num_concurrent / total_time,
            "concurrency_efficiency": len(successful_responses) / num_concurrent,
            "estimated_sequential_time": 0.5 * num_concurrent,  # Assume 0.5s per request sequentially
            "speedup_factor": (0.5 * num_concurrent) / total_time if total_time > 0 else 0
        }
        
        print(f"üìä Async Benchmark Results:")
        print(f"   Total Time: {total_time:.2f}s")
        print(f"   Requests/Second: {results['requests_per_second']:.1f}")
        print(f"   Speedup Factor: {results['speedup_factor']:.1f}x")
        print(f"   Efficiency: {results['concurrency_efficiency']:.1%}")
        
        return results
    
    async def close_async(self):
        """
        Properly close async resources
        """
        if hasattr(self.client, 'aclose'):
            await self.client.aclose()
        print("üîí AsyncGroq client closed")


# === FACTORY FUNCTIONS ===

def create_async_groq_client(api_key: str, 
                           model_type: AIModelType = AIModelType.GROQ_MIXTRAL) -> AsyncGroqClient:
    """
    Factory function for AsyncGroqClient
    
    VERWENDUNG:
    client = create_async_groq_client(api_key, AIModelType.GROQ_MIXTRAL)
    """
    return AsyncGroqClient(api_key=api_key, model_type=model_type)


def create_speed_optimized_async_client(api_key: str) -> AsyncGroqClient:
    """
    Creates async Groq client optimized for maximum speed and concurrency
    """
    client = AsyncGroqClient(
        api_key=api_key, 
        model_type=AIModelType.GROQ_LLAMA,  # Fastest model
        max_concurrent_requests=30  # Higher concurrency
    )
    return client


# === TESTING UTILITIES ===

async def test_async_groq_integration(api_key: str) -> Dict[str, Any]:
    """
    Comprehensive test of async Groq integration
    """
    
    results = {
        "connection": False,
        "async_speed_test": False,
        "batch_processing": False,
        "concurrent_recommendations": False,
        "real_time_streaming": False,
        "performance_metrics": None,
        "errors": []
    }
    
    try:
        async with create_async_groq_client(api_key, AIModelType.GROQ_MIXTRAL) as client:
            
            # Test 1: Connection
            text_response = await client.generate_text_async("Hello Async Groq!", max_tokens=20)
            results["connection"] = text_response.success
            
            if results["connection"]:
                # Test 2: Speed Benchmark
                try:
                    speed_results = await client.async_speed_benchmark_async(5)
                    results["async_speed_test"] = True
                    results["performance_metrics"] = speed_results
                except Exception as e:
                    results["errors"].append(f"Speed test failed: {e}")
                
                # Test 3: Batch Processing
                if results["async_speed_test"]:
                    try:
                        test_profile = {
                            "summary": "creative person who loves photography",
                            "personality_scores": {"openness": 0.8},
                            "hobbies": ["photography"]
                        }
                        
                        family = [
                                    {"name": "Alice", "profile": test_profile, "occasion": "birthday", "budget": "‚Ç¨50-‚Ç¨100", "relationship": "friend"},
        {"name": "Bob", "profile": test_profile, "occasion": "christmas", "budget": "‚Ç¨100-‚Ç¨150", "relationship": "family"}
                        ]
                        
                        batch_start = time.time()
                        family_gifts = await client.batch_family_gift_recommendations_async(family)
                        batch_time = time.time() - batch_start
                        
                        results["batch_processing"] = len(family_gifts) == 2
                        results["batch_time"] = batch_time
                        results["sample_batch_results"] = [gift.model_dump() for gift in family_gifts]
                        
                    except Exception as e:
                        results["errors"].append(f"Batch processing failed: {e}")
                
                # Test 4: Real-time Streaming
                if results["batch_processing"]:
                    try:
                        suggestions = await client.real_time_suggestion_stream_async(
                            test_profile, "birthday", "50-100", 3
                        )
                        results["real_time_streaming"] = len(suggestions) >= 2
                        results["sample_suggestions"] = suggestions
                        
                    except Exception as e:
                        results["errors"].append(f"Real-time streaming failed: {e}")
        
    except Exception as e:
        results["errors"].append(f"AsyncGroq client creation failed: {e}")
    
    return results


async def compare_sync_vs_async_groq_performance(sync_client, async_client, num_requests: int = 5) -> Dict[str, Any]:
    """
    Compare performance between sync and async Groq clients
    """
    
    results = {
        "sync_time": 0.0,
        "async_time": 0.0,
        "speedup_factor": 0.0,
        "sync_success_rate": 0.0,
        "async_success_rate": 0.0
    }
    
    test_prompt = "Quick creative gift idea"
    
    # Test Sync Client
    print("üêå Testing sync Groq client...")
    sync_start = time.time()
    sync_successes = 0
    
    for i in range(num_requests):
        try:
            response = sync_client.generate_text(f"{test_prompt} #{i+1}", max_tokens=50)
            if response.success:
                sync_successes += 1
        except Exception:
            pass
    
    results["sync_time"] = time.time() - sync_start
    results["sync_success_rate"] = sync_successes / num_requests
    
    # Test Async Client
    print("‚ö° Testing async Groq client...")
    async_start = time.time()
    
    async with async_client:
        tasks = [
            async_client.generate_text_async(f"{test_prompt} #{i+1}", max_tokens=50)
            for i in range(num_requests)
        ]
        
        responses = await asyncio.gather(*tasks, return_exceptions=True)
        async_successes = sum(1 for r in responses if isinstance(r, AIResponse) and r.success)
    
    results["async_time"] = time.time() - async_start
    results["async_success_rate"] = async_successes / num_requests
    
    # Calculate speedup
    if results["async_time"] > 0:
        results["speedup_factor"] = results["sync_time"] / results["async_time"]
    
    print(f"üìä Performance Comparison:")
    print(f"   Sync: {results['sync_time']:.2f}s")
    print(f"   Async: {results['async_time']:.2f}s")
    print(f"   Speedup: {results['speedup_factor']:.1f}x")
    
    return results


# === USAGE EXAMPLES ===

async def example_usage():
    """
    Examples of AsyncGroqClient usage
    """
    
    api_key = "your-groq-api-key"
    
    # Example 1: Single fast recommendation
    async with create_async_groq_client(api_key) as client:
        
        profile = {
            "summary": "creative photographer who loves travel",
            "personality_scores": {"openness": 0.8, "extraversion": 0.6},
            "hobbies": ["photography", "travel"]
        }
        
        # Async fast recommendation
        gift = await client.fast_gift_recommendation_async(
            personality_profile=profile,
            occasion="birthday",
            budget_range="‚Ç¨100-‚Ç¨200",
            relationship="close_friend"
        )
        
        print(f"‚ö° Fast recommendation: {gift.gift_name}")
    
    # Example 2: Family batch processing
    async with create_async_groq_client(api_key) as client:
        
        family = [
                    {"name": "Mom", "profile": profile, "occasion": "christmas", "budget": "‚Ç¨100-‚Ç¨200", "relationship": "mother"},
        {"name": "Dad", "profile": profile, "occasion": "christmas", "budget": "‚Ç¨75-‚Ç¨150", "relationship": "father"},
        {"name": "Sister", "profile": profile, "occasion": "christmas", "budget": "‚Ç¨50-‚Ç¨100", "relationship": "sister"}
        ]
        
        # All recommendations in parallel
        family_gifts = await client.batch_family_gift_recommendations_async(family)
        
        for gift in family_gifts:
            member_name = gift.metadata.get("family_member", "Unknown")
            print(f"üë®‚Äçüë©‚Äçüëß‚Äçüë¶ {member_name}: {gift.gift_name}")


if __name__ == "__main__":
    # Run example
    asyncio.run(example_usage())